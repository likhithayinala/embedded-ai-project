{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for 5 seconds...\n",
      "Recording saved to output.wav\n"
     ]
    }
   ],
   "source": [
    "# Record live audio \n",
    "from pyaudio import PyAudio, paInt16\n",
    "import wave\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import soundfile as sf\n",
    "import sounddevice as sd\n",
    "\n",
    "\n",
    "\n",
    "# Record live audio\n",
    "duration = 5  # seconds\n",
    "filename = \"output.wav\"\n",
    "fs = 44100  # Sample rate\n",
    "\n",
    "print(f\"Recording for {duration} seconds...\")\n",
    "recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "sd.wait()  # Wait until recording is finished\n",
    "sf.write(filename, recording, fs)\n",
    "print(f\"Recording saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/likhith/.local/lib/python3.10/site-packages (4.40.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.40.0\n",
      "    Uninstalling tqdm-4.40.0:\n",
      "      Successfully uninstalled tqdm-4.40.0\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Whisper model\n",
    "model = faster_whisper.WhisperModel(\"small\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording and transcribing in 1-second chunks for 5 seconds...\n",
      "Text:  ვვ ვვ ვვ ვვ ვვ ვვ ვ ზ ი ვ ჵ ოვ ი მ ზე\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18562/3718503497.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mchunk_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_duration\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Recording and transcribing in {chunk_duration}-second chunks for {duration} seconds...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplerate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Keep the stream open for the specified duration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;34m\"\"\"Stop and close the stream when exiting a \"with\" statement.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self, ignore_errors)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \"\"\"\n\u001b[0;32m-> 1137\u001b[0;31m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPa_StopStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Error stopping stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import faster_whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "# Define the audio stream callback\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(f\"Status: {status}\")\n",
    "    # Convert audio data to the required format\n",
    "    audio_data = indata[:, 0].astype(np.float32) / 32768.0  # Normalize int16 to float32\n",
    "    # Transcribe 1 second of audio at a time\n",
    "    segments, _ =(audio_data, beam_size=5, without_timestamps=True)\n",
    "    for segment in segments:\n",
    "        print(f\"Text: {segment.text}\")\n",
    "\n",
    "# Start the audio stream\n",
    "chunk_duration = 1  # seconds\n",
    "chunk_frames = int(chunk_duration * fs)\n",
    "print(f\"Recording and transcribing in {chunk_duration}-second chunks for {duration} seconds...\")\n",
    "with sd.InputStream(callback=audio_callback, channels=1, samplerate=fs, dtype='int16', blocksize=chunk_frames):\n",
    "    sd.sleep(duration * 1000)  # Keep the stream open for the specified duration\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USing faster whisper do text to speech\n",
    "import faster_whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "from faster_whisper import WhisperModel\n",
    "import soundfile as sf\n",
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from gtts import gTTS\n",
    "\n",
    "\n",
    "# Record live audio \n",
    "from pyaudio import PyAudio, paInt16\n",
    "import wave\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import soundfile as sf\n",
    "import sounddevice as sd\n",
    "\n",
    "\n",
    "\n",
    "# Record live audio\n",
    "duration = 5  # seconds\n",
    "filename = \"output.wav\"\n",
    "fs = 44100  # Sample rate\n",
    "\n",
    "print(f\"Recording for {duration} seconds...\")\n",
    "recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "sd.wait()  # Wait until recording is finished\n",
    "sf.write(filename, recording, fs)\n",
    "print(f\"Recording saved to {filename}\")\n",
    "\n",
    "# Load the model\n",
    "model = WhisperModel(\"small\", compute_type=\"int8\")\n",
    "# Faster-whisper is for speech-to-text (not text-to-speech)\n",
    "# For text-to-speech we need to use a dedicated TTS library\n",
    "# Let's install and use gTTS (Google Text-to-Speech)\n",
    "\n",
    "\n",
    "def text_to_speech(text, output_file=\"tts_output.mp3\"):\n",
    "    \"\"\"Convert text to speech using Google's Text-to-Speech API\"\"\"\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    tts.save(output_file)\n",
    "    print(f\"Text-to-speech saved to {output_file}\")\n",
    "    \n",
    "    # Play the generated audio\n",
    "    data, samplerate = sf.read(output_file)\n",
    "    sd.play(data, samplerate)\n",
    "    sd.wait()\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"This is a demonstration of text to speech conversion using Google's TTS service.\"\n",
    "text_to_speech(sample_text)\n",
    "\n",
    "# You can also convert transcribed text to speech\n",
    "# For example, after transcribing with faster-whisper:\n",
    "# text_to_speech(transcription_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording yes_1.wav for 1s…\n",
      "Recording yes_2.wav for 1s…\n",
      "Recording yes_3.wav for 1s…\n",
      "Recording yes_4.wav for 1s…\n",
      "Recording yes_5.wav for 1s…\n",
      "Recording not_yes_1.wav for 1s…\n",
      "Recording not_yes_2.wav for 1s…\n",
      "Recording not_yes_3.wav for 1s…\n",
      "Recording not_yes_4.wav for 1s…\n",
      "Recording not_yes_5.wav for 1s…\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import sys\n",
    "sys.path.append('/content/Torch-KWT')\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 1  # seconds\n",
    "\n",
    "def record_clip(filename):\n",
    "    print(f\"Recording {filename} for {DURATION}s…\")\n",
    "    clip = sd.rec(int(DURATION * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1)  # sounddevice rec\n",
    "    sd.wait()  # wait until recording is done\n",
    "    sf.write(filename, clip, SAMPLE_RATE)\n",
    "\n",
    "# Record five “yes” samples\n",
    "for i in range(1, 6):\n",
    "    record_clip(f\"yes_{i}.wav\")\n",
    "# (Optionally) record five “not_yes” samples\n",
    "for i in range(1, 6):\n",
    "    record_clip(f\"not_yes_{i}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "class YesNoDataset(Dataset):\n",
    "    def __init__(self, file_list, label, transform):\n",
    "        self.files = file_list\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav, sr = torchaudio.load(self.files[idx])\n",
    "        if sr != SAMPLE_RATE:\n",
    "            wav = T.Resample(sr, SAMPLE_RATE)(wav)  # resample if needed\n",
    "        spec = T.MelSpectrogram(\n",
    "            sample_rate=SAMPLE_RATE, n_fft=1024, win_length=640,\n",
    "            hop_length=160, n_mels=40\n",
    "        )(wav)  # (1, 40, T)\n",
    "        log_mel = torch.log(spec + 1e-6)\n",
    "        # pad/crop to 98 frames\n",
    "        if log_mel.shape[-1] < 98:\n",
    "            pad = 98 - log_mel.shape[-1]\n",
    "            log_mel = F.pad(log_mel, (0, pad))\n",
    "        log_mel = log_mel[:, :, :98]\n",
    "        return log_mel, torch.tensor(self.label)\n",
    "\n",
    "# Assemble datasets\n",
    "yes_files = [f\"yes_{i}.wav\" for i in range(1,6)]\n",
    "no_files  = [f\"not_yes_{i}.wav\" for i in range(1,6)]\n",
    "transform = None  # already applied above\n",
    "\n",
    "dataset = torch.utils.data.ConcatDataset([\n",
    "    YesNoDataset(yes_files, 1, transform),\n",
    "    YesNoDataset(no_files, 0, transform)\n",
    "])\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['mlp_head.0.weight', 'mlp_head.0.bias', 'mlp_head.1.weight', 'mlp_head.1.bias'], unexpected_keys=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from kwt.models.kwt import kwt_from_name, KWT\n",
    "\n",
    "# 1) Load original 35-class model to grab weights\n",
    "base_model = kwt_from_name(\"kwt-1\")  # default num_classes=35\n",
    "ckpt = torch.load(\"kwt/kwt1_v01.pth\", map_location=\"cpu\")  # your downloaded weights :contentReference[oaicite:8]{index=8}\n",
    "base_model.load_state_dict(ckpt, strict=False)\n",
    "\n",
    "# 2) Create new model for 2 classes\n",
    "cfg = {\n",
    "    \"input_res\":[40,98],\"patch_res\":[40,1],\"num_classes\":2,\n",
    "    \"mlp_dim\":256,\"dim\":64,\"heads\":1,\"depth\":12,\n",
    "    \"dropout\":0.0,\"emb_dropout\":0.1,\"pre_norm\":False\n",
    "}\n",
    "model = KWT(**cfg)\n",
    "\n",
    "# 3) Transfer matching weights (all but mlp_head)\n",
    "state_dict = base_model.state_dict()\n",
    "# remove the old head weights so shapes match\n",
    "for key in list(state_dict):\n",
    "    if key.startswith(\"mlp_head\"):\n",
    "        state_dict.pop(key)\n",
    "model.load_state_dict(state_dict, strict=False)  # ignore missing head :contentReference[oaicite:9]{index=9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7088\n",
      "Epoch 2/10, Loss: 0.6596\n",
      "Epoch 3/10, Loss: 0.7274\n",
      "Epoch 4/10, Loss: 0.7196\n",
      "Epoch 5/10, Loss: 0.5524\n",
      "Epoch 6/10, Loss: 0.6404\n",
      "Epoch 7/10, Loss: 0.5123\n",
      "Epoch 8/10, Loss: 0.5973\n",
      "Epoch 9/10, Loss: 0.4313\n",
      "Epoch 10/10, Loss: 0.3963\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)               # forward\n",
    "        loss = criterion(logits, y)     # compute loss\n",
    "        loss.backward()                 # backprop\n",
    "        optimizer.step()                # update weights\n",
    "        total_loss += loss.item()\n",
    "    avg = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg:.4f}\")\n",
    "torch.save(model.state_dict(), \"yesno_trained_model.pth\")  # save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, label = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5854,  1.0904]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "model(audio.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaking: Hello, this is an offline text-to-speech demonstration using pyttsx3.\n",
      "Voice 0: Afrikaans - ['af']\n",
      "Voice 1: Amharic - ['am']\n",
      "Voice 2: Aragonese - ['an']\n",
      "Voice 3: Arabic - ['ar']\n",
      "Voice 4: Assamese - ['as']\n",
      "Voice 5: Azerbaijani - ['az']\n",
      "Voice 6: Bashkir - ['ba']\n",
      "Voice 7: Bulgarian - ['bg']\n",
      "Voice 8: Bengali - ['bn']\n",
      "Voice 9: Bishnupriya Manipuri - ['bpy']\n",
      "Voice 10: Bosnian - ['bs']\n",
      "Voice 11: Catalan - ['ca']\n",
      "Voice 12: Chinese (Mandarin) - ['cmn']\n",
      "Voice 13: Czech - ['cs']\n",
      "Voice 14: Welsh - ['cy']\n",
      "Voice 15: Danish - ['da']\n",
      "Voice 16: German - ['de']\n",
      "Voice 17: Greek - ['el']\n",
      "Voice 18: English (Caribbean) - ['en-029']\n",
      "Voice 19: English (Great Britain) - ['en-gb']\n",
      "Voice 20: English (Scotland) - ['en-gb-scotland']\n",
      "Voice 21: English (Lancaster) - ['en-gb-x-gbclan']\n",
      "Voice 22: English (West Midlands) - ['en-gb-x-gbcwmd']\n",
      "Voice 23: English (Received Pronunciation) - ['en-gb-x-rp']\n",
      "Voice 24: English (America) - ['en-us']\n",
      "Voice 25: Esperanto - ['eo']\n",
      "Voice 26: Spanish (Spain) - ['es']\n",
      "Voice 27: Spanish (Latin America) - ['es-419']\n",
      "Voice 28: Estonian - ['et']\n",
      "Voice 29: Basque - ['eu']\n",
      "Voice 30: Persian - ['fa']\n",
      "Voice 31: Persian (Pinglish) - ['fa-latn']\n",
      "Voice 32: Finnish - ['fi']\n",
      "Voice 33: French (Belgium) - ['fr-be']\n",
      "Voice 34: French (Switzerland) - ['fr-ch']\n",
      "Voice 35: French (France) - ['fr-fr']\n",
      "Voice 36: Gaelic (Irish) - ['ga']\n",
      "Voice 37: Gaelic (Scottish) - ['gd']\n",
      "Voice 38: Guarani - ['gn']\n",
      "Voice 39: Greek (Ancient) - ['grc']\n",
      "Voice 40: Gujarati - ['gu']\n",
      "Voice 41: Hakka Chinese - ['hak']\n",
      "Voice 42: Hindi - ['hi']\n",
      "Voice 43: Croatian - ['hr']\n",
      "Voice 44: Haitian Creole - ['ht']\n",
      "Voice 45: Hungarian - ['hu']\n",
      "Voice 46: Armenian (East Armenia) - ['hy']\n",
      "Voice 47: Armenian (West Armenia) - ['hyw']\n",
      "Voice 48: Interlingua - ['ia']\n",
      "Voice 49: Indonesian - ['id']\n",
      "Voice 50: Icelandic - ['is']\n",
      "Voice 51: Italian - ['it']\n",
      "Voice 52: Japanese - ['ja']\n",
      "Voice 53: Lojban - ['jbo']\n",
      "Voice 54: Georgian - ['ka']\n",
      "Voice 55: Kazakh - ['kk']\n",
      "Voice 56: Greenlandic - ['kl']\n",
      "Voice 57: Kannada - ['kn']\n",
      "Voice 58: Korean - ['ko']\n",
      "Voice 59: Konkani - ['kok']\n",
      "Voice 60: Kurdish - ['ku']\n",
      "Voice 61: Kyrgyz - ['ky']\n",
      "Voice 62: Latin - ['la']\n",
      "Voice 63: Lingua Franca Nova - ['lfn']\n",
      "Voice 64: Lithuanian - ['lt']\n",
      "Voice 65: Latvian - ['lv']\n",
      "Voice 66: Māori - ['mi']\n",
      "Voice 67: Macedonian - ['mk']\n",
      "Voice 68: Malayalam - ['ml']\n",
      "Voice 69: Marathi - ['mr']\n",
      "Voice 70: Malay - ['ms']\n",
      "Voice 71: Maltese - ['mt']\n",
      "Voice 72: Myanmar (Burmese) - ['my']\n",
      "Voice 73: Norwegian Bokmål - ['nb']\n",
      "Voice 74: Nahuatl (Classical) - ['nci']\n",
      "Voice 75: Nepali - ['ne']\n",
      "Voice 76: Dutch - ['nl']\n",
      "Voice 77: Oromo - ['om']\n",
      "Voice 78: Oriya - ['or']\n",
      "Voice 79: Punjabi - ['pa']\n",
      "Voice 80: Papiamento - ['pap']\n",
      "Voice 81: Polish - ['pl']\n",
      "Voice 82: Portuguese (Portugal) - ['pt']\n",
      "Voice 83: Portuguese (Brazil) - ['pt-br']\n",
      "Voice 84: Pyash - ['py']\n",
      "Voice 85: K'iche' - ['quc']\n",
      "Voice 86: Romanian - ['ro']\n",
      "Voice 87: Russian - ['ru']\n",
      "Voice 88: Russian (Latvia) - ['ru-lv']\n",
      "Voice 89: Sindhi - ['sd']\n",
      "Voice 90: Shan (Tai Yai) - ['shn']\n",
      "Voice 91: Sinhala - ['si']\n",
      "Voice 92: Slovak - ['sk']\n",
      "Voice 93: Slovenian - ['sl']\n",
      "Voice 94: Albanian - ['sq']\n",
      "Voice 95: Serbian - ['sr']\n",
      "Voice 96: Swedish - ['sv']\n",
      "Voice 97: Swahili - ['sw']\n",
      "Voice 98: Tamil - ['ta']\n",
      "Voice 99: Telugu - ['te']\n",
      "Voice 100: Setswana - ['tn']\n",
      "Voice 101: Turkish - ['tr']\n",
      "Voice 102: Tatar - ['tt']\n",
      "Voice 103: Urdu - ['ur']\n",
      "Voice 104: Uzbek - ['uz']\n",
      "Voice 105: Vietnamese (Northern) - ['vi']\n",
      "Voice 106: Vietnamese (Central) - ['vi-vn-x-central']\n",
      "Voice 107: Vietnamese (Southern) - ['vi-vn-x-south']\n",
      "Voice 108: Chinese (Cantonese) - ['yue']\n",
      "Speech completed successfully: True\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "import platform\n",
    "import time\n",
    "\n",
    "def speak_text(text):\n",
    "    try:\n",
    "        # Initialize the TTS engine\n",
    "        engine = pyttsx3.init()\n",
    "        \n",
    "        # Adjust properties like rate and volume\n",
    "        rate = engine.getProperty('rate')\n",
    "        engine.setProperty('rate', rate)  # Slow down the speech\n",
    "        \n",
    "        # Set volume\n",
    "        engine.setProperty('volume', 1.0)\n",
    "        \n",
    "        # Choose a voice\n",
    "        voices = engine.getProperty('voices')\n",
    "        if voices:\n",
    "            engine.setProperty('voice', 'en-us')  # Set to a specific voice (e.g., 'en-us')\n",
    "        \n",
    "        # Speak the text\n",
    "        engine.say(text)\n",
    "        \n",
    "        # This is important - keep a reference to the engine until it's done\n",
    "        engine.runAndWait()\n",
    "        \n",
    "        # Add a small delay to ensure processing is complete\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Explicitly stop and dispose of the engine\n",
    "        engine.stop()\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error in text-to-speech: {e}\")\n",
    "        return False\n",
    "\n",
    "# Use the existing text variable\n",
    "print(f\"Speaking: {text}\")\n",
    "speak_result = speak_text(text)\n",
    "print(f\"Speech completed successfully: {speak_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowing camera to adjust...\n",
      "Detected 3 face(s) for blurring\n",
      "Saved image with blurred faces as 'blurred_image.png'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time \n",
    "# Load the image\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Give camera time to adjust to lighting conditions\n",
    "print(\"Allowing camera to adjust...\")\n",
    "for _ in range(10):  # Capture and discard frames to let camera adjust\n",
    "    cap.read()\n",
    "    time.sleep(0.1)  # Short delay between frames\n",
    "\n",
    "# Now capture the actual frame we want to use\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "# Check if image was loaded properly\n",
    "cv2.imwrite(\"data/user_image.jpg\", frame)\n",
    "image = frame\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, minNeighbors=3)\n",
    "print(f\"Detected {len(faces)} face(s) for blurring\")\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    face_region = image[y:y+h, x:x+w]\n",
    "    blurred_face = cv2.GaussianBlur(face_region, (99, 99), 30)\n",
    "    image[y:y+h, x:x+w] = blurred_face\n",
    "\n",
    "# Save the image with blurred faces\n",
    "cv2.imwrite('data/user_image_blurred.png', image)\n",
    "print(\"Saved image with blurred faces as 'blurred_image.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
